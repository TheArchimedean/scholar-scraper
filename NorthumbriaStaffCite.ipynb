{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73d4bb56-7c0d-4af4-a9d9-99a7d03b3b52",
   "metadata": {},
   "source": [
    "# Google Scholar Data for Recruitment at Northumbria University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c1c3a2-51df-4694-b009-adeb0cbf629b",
   "metadata": {},
   "source": [
    "This notebook seeks to scrape data from Google scholar about potential new staff at the university, garnering information such as their name, citations per year, and a list of their papers and associated number of citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910368b6-51dd-4ffe-9a2d-588809cef334",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d13a2d-621c-4a80-8ebb-dde144eaa9d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install pandas numpy scipy tqdm selenium beautifulsoup4 pyodbc colorama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81f12349-0ccf-47b0-ab0a-332b1377d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import pyodbc\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm \n",
    "from colorama import init as colorama_init\n",
    "from colorama import Fore\n",
    "from colorama import Style\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os.path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2695292a-b79d-4c46-830a-b769c49454be",
   "metadata": {},
   "source": [
    "## Scraping Google Scholar with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574a92de-f8e2-423d-9fd9-727fae17bfb0",
   "metadata": {},
   "source": [
    "On Google Scholar, one can view a profile of an author to see a list of all their papers and citations. There is a base URL that does this and names can be entered as a seach query in the URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fd230d-617c-4a3d-a9f0-771744723db5",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7368d0d-30c3-4077-b158-361216b4ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scholar_search_url(search):\n",
    "    '''\n",
    "    Gets the Google Scholar search results for user inputted first and last names \n",
    "\n",
    "    Parameters:\n",
    "        first_name (str): first name\n",
    "        last_name (str): last name\n",
    "\n",
    "    Returns:\n",
    "        search_url (str): url of search query\n",
    "    '''\n",
    "    s = search.split(\" \")\n",
    "    query = \"\"\n",
    "    for i in s:\n",
    "        query = query + \"+\" + i\n",
    "\n",
    "    query = query[1:]\n",
    "\n",
    "    url = \"https://scholar.google.com/citations?hl=en&view_op=search_authors&mauthors=\" + query + \"&btnG=\"\n",
    "    \n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79d3c902-f6cd-48a8-80d2-81a9513dbe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_id(s):\n",
    "    '''\n",
    "    Generates a unique hash id for each column based on a string input which will be the paper title. \n",
    "\n",
    "    Parameters:\n",
    "        s (str): a string input, which will be made up of multiple columns of a dataframe 'added' (concatenated) together\n",
    "\n",
    "    Returns:\n",
    "        An 8 digit hash that uniquely identifies the string entered, and hence the row of a dataframe it represents\n",
    "    '''\n",
    "    return int(hashlib.sha256(s.encode('utf-8')).hexdigest(), 16) % 10**8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "255ce528-fc82-496a-8e86-c7e6e14913ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date(s):\n",
    "    '''\n",
    "    If a date does not contian a '/', it is likely in the form YYYY, so add the following '/1/1' to make compatable with datetime YYYY/mm/dd\n",
    "\n",
    "    Parameters:\n",
    "        s (str or numerical type): a string or numerical form of a date\n",
    "\n",
    "    Returns:\n",
    "        A date representing s in the form YYYY/mm/dd.\n",
    "    '''\n",
    "    s= str(s)\n",
    "    n = s.count('/')\n",
    "    \n",
    "    if n == 2:\n",
    "        return s \n",
    "    elif n == 1:\n",
    "        return s + '/1'\n",
    "    else:\n",
    "        return s + '/1/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed0a7d87-cc28-47d5-98c6-5f2e5316c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sleep(min, max):\n",
    "    '''\n",
    "    Instead of sleeping for a set time, this function sleeps for a random time defined by min and max\n",
    "\n",
    "    Parameters:\n",
    "        min (float): minimum time to sleep\n",
    "        max (float): maximum time to sleep\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    #Time-to-sleep\n",
    "    tts = np.random.random() #random number between 0 and 1\n",
    "    tts *= (max - min) #now between 0 and (max-min)\n",
    "    tts += min #now between min and max\n",
    "\n",
    "    time.sleep(tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ec7d3ee-a30e-4f3c-880d-a2c1e78de943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_table(soup):\n",
    "    '''\n",
    "    Get's the table of academic papers from the html data. This contains their 'Title', number of 'Citations', 'Year' of publication, and the individual 'Paper url'\n",
    "    which will be used by a later function to gather further details of the paper.\n",
    "\n",
    "    Parameters:\n",
    "        soup (bs4): parsed html data for the academics page\n",
    "    '''\n",
    "    \n",
    "    l = list() #list to hold paper dictionaries \n",
    "    o = {} #dictionary for each paper\n",
    "\n",
    "    allPapersContainer = soup.find(\"table\",{\"id\":\"gsc_a_t\"}) #find the table of papers\n",
    "    allPapers = allPapersContainer.find_all(\"tr\") #extract all papers from within that panel \n",
    "    \n",
    "    for paper in allPapers:\n",
    "        #print(paper)\n",
    "        #title\n",
    "        try:\n",
    "            o[\"Title\"]=paper.find(\"a\",{\"class\":\"gsc_a_at\"}).text\n",
    "        except:\n",
    "            o[\"Title\"]=None\n",
    "\n",
    "        #year\n",
    "        try:\n",
    "            o[\"Citations\"]=paper.find(\"a\",{\"class\":\"gsc_a_ac gs_ibl\"}).text\n",
    "        except:\n",
    "            o[\"Citations\"]=None \n",
    "    \n",
    "        #citations\n",
    "        try:\n",
    "            o[\"Year\"]=paper.find(\"span\",{\"class\":\"gsc_a_h gsc_a_hc gs_ibl\"}).text\n",
    "        except:\n",
    "            o[\"Year\"]=None\n",
    "\n",
    "        #paper-url\n",
    "        try:\n",
    "            search = paper.find(\"a\",\"gsc_a_at\").get(\"href\")\n",
    "            o[\"Paper url\"]= \"https://scholar.google.com\" + search\n",
    "        except:\n",
    "            o[\"Paper url\"]=None\n",
    "            \n",
    "        l.append(o)\n",
    "        o={}\n",
    "        \n",
    "    paper_df = pd.DataFrame(l) #list of dictionaries to a dataframe\n",
    "    paper_df.dropna(axis=0, inplace=True, ignore_index=True)\n",
    "    print(f'A dataframe of papers has been successfully stored.')\n",
    "\n",
    "    return paper_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3715995-765e-4beb-886a-56a45040f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_academic_table(soup):\n",
    "    '''\n",
    "    Get's the academics information from the html data. This will grab their name as 'Academic', university 'Affiliation', their total number of 'Citations', \n",
    "    their 'h-index' and 'i10-index', and then will breakdown their citations per year as far back as they are stored on Google Scholar.\n",
    "\n",
    "    Parameters:\n",
    "        soup (bs4): parsed html data for the academics page\n",
    "    '''\n",
    "    \n",
    "    academic_df = pd.DataFrame(columns = ['Academic', 'Affiliation', 'Citations','h-index','i10-index'], index=[0])\n",
    "\n",
    "    #Grabs the name and university affiliation from the top of the page (deals with capitalization, full names etc\n",
    "    academic_df['Academic'][0] = soup.find(\"div\",{\"id\":\"gsc_prf_inw\"}).text\n",
    "    academic_df['Affiliation'][0] = soup.find(\"a\",{\"class\":\"gsc_prf_ila\"}).text\n",
    "\n",
    "    #Finds the html data associated with the info panel on the right\n",
    "    sidePanel = soup.find(\"div\",{\"id\":\"gsc_rsb_cit\"})\n",
    "    table = sidePanel.find(\"tbody\") #table with total citations\n",
    "    rows = table.find_all(\"tr\")\n",
    "\n",
    "    for row in rows:\n",
    "        field = row.find(\"a\",{\"class\":\"gsc_rsb_f gs_ibl\"}).text\n",
    "        \n",
    "        try:\n",
    "            academic_df[field][0] = row.find(\"td\",{\"class\":\"gsc_rsb_std\"}).text \n",
    "        except:\n",
    "            academic_df[field][0] = None\n",
    "            \n",
    "\n",
    "    #Gets citations per year from the hover-over info on the histogram\n",
    "    #####\n",
    "    #There is a complication here because not every year has citations, and year labels and values are not intrinsically linked.\n",
    "    #Thus need to ensure the lists are the same length, and pair them up\n",
    "    hist_labs = sidePanel.find_all(\"span\", {\"class\":\"gsc_g_t\"})\n",
    "    hist_vals = sidePanel.find_all(\"a\", {\"class\":\"gsc_g_a\"}) \n",
    "\n",
    "    min_year = int(hist_labs[0].text)\n",
    "    max_year = int(hist_labs[-1].text)\n",
    "    num_years = int(max_year-min_year+1)\n",
    "\n",
    "    \n",
    "    #Years from min_year to max_year are indexed in the html from 28 to 1\n",
    "    #If an index value is not present, then 0 citations\n",
    "    #This loop stores only the index values for years with citation.\n",
    "    real_year_index = list()\n",
    "    for val in hist_vals:\n",
    "        year_index = val.get(\"style\")\n",
    "        year_index = year_index.split(':')[-1] #splits style value at every : and keeps only the last one, the index\n",
    "        real_year_index.append(int(year_index))    \n",
    "\n",
    "    \n",
    "    #List of all possible indices to compare against\n",
    "    all_possible_indices = list()\n",
    "    for x in reversed(range(num_years)):\n",
    "        all_possible_indices.append(x+1)\n",
    "        \n",
    "\n",
    "    #Creating a binary list the length of all_possible_indices. If there WAS an index in the html, assign 1. If not, 0\n",
    "    has_citations = list()\n",
    "    for possible_index in all_possible_indices:\n",
    "        if possible_index in real_year_index:\n",
    "            has_citations.append(1)\n",
    "        else:\n",
    "            has_citations.append(0)\n",
    "\n",
    "    \n",
    "    #Now can run through the binary array and if 1, can assign relavnt value and if 0, will skip.\n",
    "    #Will count through the histogram values inside the loop, since that list is smaller (as missing values were omitted)\n",
    "    hist_val_counter = 0\n",
    "    for idx, item in enumerate(has_citations):\n",
    "        if item == 1:\n",
    "            has_citations[idx] = hist_vals[hist_val_counter].text\n",
    "            hist_val_counter += 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    \n",
    "    #Can now finally assign a column and value for each year and citation count\n",
    "    for year, cite_count in zip(hist_labs, has_citations):\n",
    "        academic_df[\"Citations in \" + str(year.text)] = np.nan #initialize\n",
    "        academic_df[\"Citations in \" + str(year.text)] = cite_count\n",
    "        \n",
    "    \n",
    "    print(f'Total citation details have been successully stored.')\n",
    "\n",
    "    \n",
    "    return academic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64f33e0f-520a-4877-8ee6-74bbd85b6e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_details(paper_df, driver):\n",
    "    '''\n",
    "    Using the paper urls from the passed dataframe, gets additional details from this url using the passed webdriver and adds them to the paper_df dataframe. These\n",
    "    additional details are: Google Scholar profile name, Publication date, Journal/Source/Conference (an entry will fit in only one of these catagories), Authors,\n",
    "    Primary author, and Supporting authors.\n",
    "\n",
    "    Parameters:\n",
    "        paper_df (pandas df): dataframe of paper information\n",
    "        driver (selenium): webdriver \n",
    "\n",
    "    Returns:\n",
    "        detailed_paper_df (pandas df): original dataframe with additional columns\n",
    "    '''\n",
    "    \n",
    "    print('Additional information will now be gathered for each paper.')\n",
    "\n",
    "    #Copy dataframe as to not overwrite data\n",
    "    detailed_paper_df = paper_df.copy()\n",
    "    \n",
    "    # Want to obtain additonal info for each paper inc. primary author, coauthors, journal\n",
    "    detailed_paper_df['Google Scholar profile name'] = np.nan\n",
    "    detailed_paper_df['Publication date'] = np.nan\n",
    "    detailed_paper_df['Journal'] = np.nan\n",
    "    detailed_paper_df['Source'] = np.nan\n",
    "    detailed_paper_df['Conference'] = np.nan\n",
    "    detailed_paper_df['Authors'] = np.nan\n",
    "    detailed_paper_df['Primary author'] = np.nan\n",
    "    detailed_paper_df['Supporting authors'] = np.nan\n",
    "    \n",
    "    for idx, paper_url in enumerate(tqdm(paper_df['Paper url'], \"Papers\", leave=None)):\n",
    "        driver.get(paper_url) #load webpage of paper\n",
    "        random_sleep(1,6)\n",
    "\n",
    "        resp = driver.page_source #grab html\n",
    "        soup=BeautifulSoup(resp,'html.parser') #parse with bs4\n",
    "\n",
    "        #Gets academics name from top of page. For consistency, will add as a feature so that if data is concatenated into a big set, \n",
    "        #it's easy to tell where the paper came from.\n",
    "        name_body = soup.find(\"div\",{\"class\":\"gs_bdy_sb_sec\"})\n",
    "        name = name_body.find_all(\"a\")[1].text\n",
    "        detailed_paper_df['Google Scholar profile name'] = name\n",
    "        #print(f'{Fore.RED}TEST: {name}{Style.RESET_ALL}')\n",
    "        \n",
    "\n",
    "        #This for loop deals with collecting the additional information from the articles specific page\n",
    "        page_body = soup.find(\"div\",{\"id\":\"gsc_vcpb\"})\n",
    "        content = page_body.find_all(\"div\",{\"class\":\"gs_scl\"})\n",
    "        \n",
    "        for item in content:\n",
    "            field = item.find(\"div\",{\"class\":\"gsc_oci_field\"}).text #gets the name of the field \n",
    "\n",
    "            if field in ['Publication date', 'Journal', 'Authors', 'Source', 'Conference']: #ignore fields that are not in this list\n",
    "                try:\n",
    "                    detailed_paper_df[field][idx] = item.find(\"div\",{\"class\":\"gsc_oci_value\"}).text\n",
    "                        \n",
    "                except:\n",
    "                    detailed_paper_df[field][idx] = 0\n",
    "            \n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        #To prevent errors with lack of author, i.e. for a patent\n",
    "        try:\n",
    "            detailed_paper_df['Authors'][idx] = detailed_paper_df['Authors'][idx].split(', ') #split authors string into an actual list of authors\n",
    "            detailed_paper_df['Primary author'][idx] = detailed_paper_df['Authors'][idx][0] #first author is primary\n",
    "        except:\n",
    "            detailed_paper_df['Authors'][idx] = np.nan\n",
    "            detailed_paper_df['Primary author'][idx] = np.nan\n",
    "\n",
    "        #Prevents error if paper has only one author\n",
    "        try:\n",
    "            if len(detailed_paper_df['Authors'][idx]) > 1:\n",
    "                detailed_paper_df['Supporting authors'][idx] = detailed_paper_df['Authors'][idx][1:] #supporting are non first\n",
    "        except:\n",
    "            detailed_paper_df['Supporting authors'][idx] = np.nan\n",
    "            \n",
    "\n",
    "    print(\"Additional details gathered.\")\n",
    "\n",
    "    return detailed_paper_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42634af4-dbef-4fe9-9508-272d5d297090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dfs(paper_df, academic_df):\n",
    "    '''\n",
    "    Appropriately formats the paper and academic dataframes. It standardises the publication date, so it is always in the form YYYY/mm/dd, generates a unique id for each\n",
    "    paper and academic using a hash created from multiple columns in the data, and corrects the discrepancy in formatting for a lack of citations, from \"\" to np.nan. \n",
    "\n",
    "    Parameters:\n",
    "        paper_df (pandas df): dataframe containing paper info\n",
    "        academic_df (pandas df): dataframe containing academic info\n",
    "\n",
    "    Returns:\n",
    "        formatted_paper_df (pandas df)\n",
    "        formatted_academic_df (pandas df)\n",
    "    '''\n",
    "\n",
    "    print(\"Formatting...\")\n",
    "\n",
    "    #Copy dataframes as to not overwrite data\n",
    "    formatted_paper_df = paper_df.copy()\n",
    "    formatted_academic_df = academic_df.copy()\n",
    "\n",
    "    #Standardise 'Publication date'\n",
    "    formatted_paper_df['Publication date'] = formatted_paper_df['Publication date'].apply(format_date)\n",
    "    \n",
    "    #Generate unique IDs\n",
    "    try:\n",
    "        formatted_paper_df['PaperID'] = formatted_paper_df[['Title', 'Publication date', 'Primary author']].sum(axis=1).apply(generate_id)\n",
    "    except:\n",
    "        formatted_paper_df['PaperID'] = formatted_paper_df[['Title', 'Publication date']].sum(axis=1).apply(generate_id) #incase of no author\n",
    "        \n",
    "    formatted_academic_df['AcademicID'] = formatted_academic_df[['Academic', 'Affiliation']].sum(axis=1).apply(generate_id)\n",
    "\n",
    "    #Changing lack of citations to correctly be nan\n",
    "    formatted_paper_df[formatted_paper_df['Citations'] == '']['Citations'] = np.nan\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    \n",
    "    return formatted_paper_df, formatted_academic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c827f3a-152d-4fb8-bccc-4af8a11353b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_show_more(driver):\n",
    "    '''\n",
    "    Ensures all papers are shown on the page by scrolling to the bottom and clicking \"show more\" until it cannot anymore, i.e. all papers are shown\n",
    "\n",
    "    Parameters:\n",
    "        driver (selenium): web driver\n",
    "    '''\n",
    "\n",
    "    while 1==1:\n",
    "        temp = driver.page_source #to check if the html button is disabled\n",
    "        \n",
    "        show_more = driver.find_element(By.ID, \"gsc_bpf_more\")\n",
    "        if show_more.is_enabled():\n",
    "            ActionChains(driver)\\\n",
    "                .scroll_to_element(show_more)\\\n",
    "                .perform()\n",
    "    \n",
    "            time.sleep(1)\n",
    "            show_more.click()\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            print(\"All papers loaded\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3666f960-b858-49e7-98cd-050cc3d5d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def searches_from_file(filename):\n",
    "    '''\n",
    "    Reads multiple search queries to be gathered from a text file named 'filename'. It reads line by line and stores each as an entry in a list, which is returned.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): name of text file containing Google Scholar profile search queries\n",
    "\n",
    "    Returns:\n",
    "        search_list (list str): list of strings containing search queries\n",
    "    '''\n",
    "\n",
    "    search_list = list()\n",
    "    \n",
    "    with open(filename) as file:\n",
    "        Lines = file.readlines()\n",
    "        for line in Lines:\n",
    "            search_list.append(line.strip())\n",
    "\n",
    "    return search_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7024dc5-ccb0-48f9-81a0-81c8a8cf0eef",
   "metadata": {},
   "source": [
    "## Driver function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c724a506-b6b1-4757-ab78-f068a071edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_by_query(search): #add name + university \n",
    "    '''\n",
    "    Using selenium, scrapes html data from Google Scholar url, parses with bs4, and then extracts information for the Paper and Academic tables, which are stored in a pandas df. \n",
    "    This is the core function, at almost the highest level. It will be used in a loop to gather data for multiple search queries.\n",
    "\n",
    "    Parameters:\n",
    "        first_name (str): first name\n",
    "        last_name (str): last name\n",
    "\n",
    "    Returns:\n",
    "        paper_df (pandas df): a dataframe containing paper information\n",
    "        academic_df (pandas df): a dataframe containing academic information\n",
    "    '''\n",
    "\n",
    "    url = get_scholar_search_url(search) #gets Google Scholar url for searched name\n",
    "    \n",
    "    driver=webdriver.Chrome() #open chromium web driver\n",
    "    print(\"Chrome driver successfully launched\")\n",
    "    \n",
    "    driver.get(url) #load desired url with chromium\n",
    "    driver.maximize_window() #maximize window\n",
    "    time.sleep(1) #wait 1s to ensure webpage fully loads\n",
    "\n",
    "    #This loads to the Google Scholar seach page. The results are shown on the page with the most relevant user at the top, which we click\n",
    "    driver.find_element(By.CLASS_NAME, 'gs_ai_pho').click() \n",
    "\n",
    "    #Ensure all articles are listed by scrolling to the bottom of the page and clicking \"Show More\"\n",
    "    click_show_more(driver)\n",
    "\n",
    "    #Now scrape all html data and parse with bs4\n",
    "    resp = driver.page_source \n",
    "    soup=BeautifulSoup(resp,'html.parser')\n",
    "\n",
    "    \n",
    "    # GET PAPER TABLE \n",
    "    paper_df = get_paper_table(soup)\n",
    "\n",
    "    \n",
    "    # GET ACADEMIC TABLE \n",
    "    academic_df = get_academic_table(soup)\n",
    "\n",
    "    \n",
    "    # GET PAPER DETAILS \n",
    "    paper_df = get_paper_details(paper_df, driver)\n",
    "\n",
    "    \n",
    "    driver.close() #close chromium driver\n",
    "    print(\"Chrome driver successfully closed\")\n",
    "\n",
    "    \n",
    "    # APPROPRIATE FORMATTING\n",
    "    f_paper_df, f_academic_df = format_dfs(paper_df, academic_df)\n",
    "\n",
    "    \n",
    "    # EXPORT DATAFRAMES TO CSV\n",
    "    directory = os.getcwd()\n",
    "    f_paper_df.to_csv(os.path.join(directory, 'Individual sheets', f'{search}_papers.csv'), index=False, index_label=False)\n",
    "    f_academic_df.to_csv(os.path.join(directory, 'Individual sheets', f'{search}_info.csv', index=False, index_label=False))\n",
    "\n",
    "    print(\"Dataframes exported to csv format.\")\n",
    "    \n",
    "    return f_paper_df, f_academic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09d30e27-088c-4bfe-bb85-68f4d1ecd566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\n",
      "Profile search: \u001b[32mrichard binns northumbria\u001b[0m\n",
      "\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\n",
      "Chrome driver successfully launched\n",
      "All papers loaded\n",
      "A dataframe of papers has been successfully stored.\n",
      "Total citation details have been successully stored.\n",
      "Additional information will now be gathered for each paper.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Papers: 100%|██████████████████████████████████████████████████████████████████████████| 84/84 [05:13<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional details gathered.\n",
      "Chrome driver successfully closed\n",
      "Formatting...\n",
      "Done!\n",
      "Dataframes exported to csv format.\n",
      "\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\n",
      "Profile search: \u001b[32mclare watt northumbria\u001b[0m\n",
      "\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\n",
      "Chrome driver successfully launched\n",
      "All papers loaded\n",
      "A dataframe of papers has been successfully stored.\n",
      "Total citation details have been successully stored.\n",
      "Additional information will now be gathered for each paper.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Papers: 100%|████████████████████████████████████████████████████████████████████████| 223/223 [13:52<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional details gathered.\n",
      "Chrome driver successfully closed\n",
      "Formatting...\n",
      "Done!\n",
      "Dataframes exported to csv format.\n",
      "\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\n",
      "Profile search: \u001b[32mmartin bees york\u001b[0m\n",
      "\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\n",
      "Chrome driver successfully launched\n",
      "All papers loaded\n",
      "A dataframe of papers has been successfully stored.\n",
      "Total citation details have been successully stored.\n",
      "Additional information will now be gathered for each paper.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Papers: 100%|██████████████████████████████████████████████████████████████████████████| 81/81 [04:46<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional details gathered.\n",
      "Chrome driver successfully closed\n",
      "Formatting...\n",
      "Done!\n",
      "Dataframes exported to csv format.\n",
      "\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\n",
      "Profile search: \u001b[32mhenning bostelmann york\u001b[0m\n",
      "\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\n",
      "Chrome driver successfully launched\n",
      "All papers loaded\n",
      "A dataframe of papers has been successfully stored.\n",
      "Total citation details have been successully stored.\n",
      "Additional information will now be gathered for each paper.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Papers: 100%|██████████████████████████████████████████████████████████████████████████| 26/26 [01:35<00:00,  3.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional details gathered.\n",
      "Chrome driver successfully closed\n",
      "Formatting...\n",
      "Done!\n",
      "Dataframes exported to csv format.\n",
      "\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\n",
      "Exporting searches_papers.csv and searches_academics.csv with all search queries...\n",
      "\u001b[32mALL TASKS COMPLETE\u001b[0m\n",
      "\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\u001b[32m─\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Initialize dataframes\n",
    "paper_df = pd.DataFrame()\n",
    "academic_df = pd.DataFrame()\n",
    "\n",
    "#Generate search list\n",
    "Searches = searches_from_file(\"search queries.txt\")\n",
    "\n",
    "for search in Searches:\n",
    "    print(f'{Fore.GREEN}─{Style.RESET_ALL}' * 50) \n",
    "    print(f'Profile search: {Fore.GREEN}{search}{Style.RESET_ALL}')\n",
    "    print(f'{Fore.GREEN}─{Style.RESET_ALL}' * 50) \n",
    "\n",
    "    try:\n",
    "        p_df, a_df = dfs_by_query(search)\n",
    "    except:\n",
    "        print(f'{Fore.RED}Cannot find a profile matching {Style.RESET_ALL}{search}')\n",
    "        continue\n",
    "    \n",
    "    paper_df = pd.concat([paper_df, p_df], join='outer', axis=0)\n",
    "    academic_df = pd.concat([academic_df, a_df], join='outer', axis=0)\n",
    "\n",
    "print(f'{Fore.GREEN}─{Style.RESET_ALL}' * 50) \n",
    "print(\"Exporting searches_papers.csv and searches_academics.csv with all search queries...\")\n",
    "paper_df.to_csv(\"searches_papers.csv\", index=False, index_label=False)\n",
    "academic_df.to_csv(\"searches_academics.csv\", index=False, index_label=False)\n",
    "\n",
    "print(f'{Fore.GREEN}ALL TASKS COMPLETE{Style.RESET_ALL}')\n",
    "print(f'{Fore.GREEN}─{Style.RESET_ALL}' * 50) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5810d53c-a72b-4df9-885e-2511d1b0c6eb",
   "metadata": {},
   "source": [
    "## Looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3d44855-cec7-4db9-9237-503063445605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Citations</th>\n",
       "      <th>Year</th>\n",
       "      <th>Paper url</th>\n",
       "      <th>Google Scholar profile name</th>\n",
       "      <th>Publication date</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Source</th>\n",
       "      <th>Conference</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Primary author</th>\n",
       "      <th>Supporting authors</th>\n",
       "      <th>PaperID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wind-driven optimization technique for estimation of solar photovoltaic parameters</td>\n",
       "      <td>75</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=FvXKYFQAAAAJ&amp;citation_for_view=FvXKYFQAAAAJ:roLk4NBRz8UC</td>\n",
       "      <td>Richard Binns</td>\n",
       "      <td>2017/12/7</td>\n",
       "      <td>IEEE Journal of Photovoltaics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Derick Mathew, Chinnappa Rani, Muthu Rajesh Kumar, Yue Wang, Richard Binns, Krishna Busawon]</td>\n",
       "      <td>Derick Mathew</td>\n",
       "      <td>[Chinnappa Rani, Muthu Rajesh Kumar, Yue Wang, Richard Binns, Krishna Busawon]</td>\n",
       "      <td>33461872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robust neural network fault estimation approach for nonlinear dynamic systems with applications to wind turbine systems</td>\n",
       "      <td>71</td>\n",
       "      <td>2019</td>\n",
       "      <td>https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=FvXKYFQAAAAJ&amp;citation_for_view=FvXKYFQAAAAJ:IWHjjKOFINEC</td>\n",
       "      <td>Richard Binns</td>\n",
       "      <td>2019/1/17</td>\n",
       "      <td>IEEE Transactions on Industrial Informatics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Reihane Rahimilarki, Zhiwei Gao, Aihua Zhang, Richard Binns]</td>\n",
       "      <td>Reihane Rahimilarki</td>\n",
       "      <td>[Zhiwei Gao, Aihua Zhang, Richard Binns]</td>\n",
       "      <td>24993085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indoor visible light communication localization system utilizing received signal strength indication technique and trilateration method</td>\n",
       "      <td>40</td>\n",
       "      <td>2018</td>\n",
       "      <td>https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=FvXKYFQAAAAJ&amp;citation_for_view=FvXKYFQAAAAJ:_kc_bZDykSQC</td>\n",
       "      <td>Richard Binns</td>\n",
       "      <td>2018/1/1</td>\n",
       "      <td>Optical Engineering</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Farag IK Mousa, Noor Almaadeed, Krishna Busawon, Ahmed Bouridane, Richard Binns, Ian Elliot]</td>\n",
       "      <td>Farag IK Mousa</td>\n",
       "      <td>[Noor Almaadeed, Krishna Busawon, Ahmed Bouridane, Richard Binns, Ian Elliot]</td>\n",
       "      <td>84642498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ultrasensitive biosensor based on magnetic microspheres enhanced microfiber interferometer</td>\n",
       "      <td>28</td>\n",
       "      <td>2019</td>\n",
       "      <td>https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=FvXKYFQAAAAJ&amp;citation_for_view=FvXKYFQAAAAJ:j3f4tGmQtD8C</td>\n",
       "      <td>Richard Binns</td>\n",
       "      <td>2019/12/1</td>\n",
       "      <td>Biosensors and Bioelectronics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Rahul Kumar, Yuankui Leng, Bin Liu, Jun Zhou, Liyang Shao, Jinhui Yuan, Xinyu Fan, Shengpeng Wan, Tao Wu, Juan Liu, Richard Binns, Yong Qing Fu, Wai Pang Ng, Gerald Farrell, Yuliya Semenova, Hengyi Xu, Yonghua Xiong, Xingdao He, Qiang Wu]</td>\n",
       "      <td>Rahul Kumar</td>\n",
       "      <td>[Yuankui Leng, Bin Liu, Jun Zhou, Liyang Shao, Jinhui Yuan, Xinyu Fan, Shengpeng Wan, Tao Wu, Juan Liu, Richard Binns, Yong Qing Fu, Wai Pang Ng, Gerald Farrell, Yuliya Semenova, Hengyi Xu, Yonghua Xiong, Xingdao He, Qiang Wu]</td>\n",
       "      <td>60436808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A rapid and controllable acoustothermal microheater using thin film surface acoustic waves</td>\n",
       "      <td>26</td>\n",
       "      <td>2021</td>\n",
       "      <td>https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=FvXKYFQAAAAJ&amp;citation_for_view=FvXKYFQAAAAJ:NMxIlDl6LWMC</td>\n",
       "      <td>Richard Binns</td>\n",
       "      <td>2021/2/1</td>\n",
       "      <td>Sensors and Actuators A: Physical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Yong Wang, Qian Zhang, Ran Tao, Dongyang Chen, Jin Xie, Hamdi Torun, Linzi E Dodd, Jingting Luo, Chen Fu, Jethro Vernon, Pep Canyelles-Pericas, Richard Binns, Yongqing Fu]</td>\n",
       "      <td>Yong Wang</td>\n",
       "      <td>[Qian Zhang, Ran Tao, Dongyang Chen, Jin Xie, Hamdi Torun, Linzi E Dodd, Jingting Luo, Chen Fu, Jethro Vernon, Pep Canyelles-Pericas, Richard Binns, Yongqing Fu]</td>\n",
       "      <td>95657531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     Title  \\\n",
       "0                                                       Wind-driven optimization technique for estimation of solar photovoltaic parameters   \n",
       "1                  Robust neural network fault estimation approach for nonlinear dynamic systems with applications to wind turbine systems   \n",
       "2  Indoor visible light communication localization system utilizing received signal strength indication technique and trilateration method   \n",
       "3                                               Ultrasensitive biosensor based on magnetic microspheres enhanced microfiber interferometer   \n",
       "4                                               A rapid and controllable acoustothermal microheater using thin film surface acoustic waves   \n",
       "\n",
       "  Citations  Year  \\\n",
       "0        75  2017   \n",
       "1        71  2019   \n",
       "2        40  2018   \n",
       "3        28  2019   \n",
       "4        26  2021   \n",
       "\n",
       "                                                                                                                        Paper url  \\\n",
       "0  https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FvXKYFQAAAAJ&citation_for_view=FvXKYFQAAAAJ:roLk4NBRz8UC   \n",
       "1  https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FvXKYFQAAAAJ&citation_for_view=FvXKYFQAAAAJ:IWHjjKOFINEC   \n",
       "2  https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FvXKYFQAAAAJ&citation_for_view=FvXKYFQAAAAJ:_kc_bZDykSQC   \n",
       "3  https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FvXKYFQAAAAJ&citation_for_view=FvXKYFQAAAAJ:j3f4tGmQtD8C   \n",
       "4  https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FvXKYFQAAAAJ&citation_for_view=FvXKYFQAAAAJ:NMxIlDl6LWMC   \n",
       "\n",
       "  Google Scholar profile name Publication date  \\\n",
       "0               Richard Binns        2017/12/7   \n",
       "1               Richard Binns        2019/1/17   \n",
       "2               Richard Binns         2018/1/1   \n",
       "3               Richard Binns        2019/12/1   \n",
       "4               Richard Binns         2021/2/1   \n",
       "\n",
       "                                       Journal Source Conference  \\\n",
       "0                IEEE Journal of Photovoltaics    NaN        NaN   \n",
       "1  IEEE Transactions on Industrial Informatics    NaN        NaN   \n",
       "2                          Optical Engineering    NaN        NaN   \n",
       "3                Biosensors and Bioelectronics    NaN        NaN   \n",
       "4            Sensors and Actuators A: Physical    NaN        NaN   \n",
       "\n",
       "                                                                                                                                                                                                                                           Authors  \\\n",
       "0                                                                                                                                                    [Derick Mathew, Chinnappa Rani, Muthu Rajesh Kumar, Yue Wang, Richard Binns, Krishna Busawon]   \n",
       "1                                                                                                                                                                                    [Reihane Rahimilarki, Zhiwei Gao, Aihua Zhang, Richard Binns]   \n",
       "2                                                                                                                                                    [Farag IK Mousa, Noor Almaadeed, Krishna Busawon, Ahmed Bouridane, Richard Binns, Ian Elliot]   \n",
       "3  [Rahul Kumar, Yuankui Leng, Bin Liu, Jun Zhou, Liyang Shao, Jinhui Yuan, Xinyu Fan, Shengpeng Wan, Tao Wu, Juan Liu, Richard Binns, Yong Qing Fu, Wai Pang Ng, Gerald Farrell, Yuliya Semenova, Hengyi Xu, Yonghua Xiong, Xingdao He, Qiang Wu]   \n",
       "4                                                                     [Yong Wang, Qian Zhang, Ran Tao, Dongyang Chen, Jin Xie, Hamdi Torun, Linzi E Dodd, Jingting Luo, Chen Fu, Jethro Vernon, Pep Canyelles-Pericas, Richard Binns, Yongqing Fu]   \n",
       "\n",
       "        Primary author  \\\n",
       "0        Derick Mathew   \n",
       "1  Reihane Rahimilarki   \n",
       "2       Farag IK Mousa   \n",
       "3          Rahul Kumar   \n",
       "4            Yong Wang   \n",
       "\n",
       "                                                                                                                                                                                                                   Supporting authors  \\\n",
       "0                                                                                                                                                      [Chinnappa Rani, Muthu Rajesh Kumar, Yue Wang, Richard Binns, Krishna Busawon]   \n",
       "1                                                                                                                                                                                            [Zhiwei Gao, Aihua Zhang, Richard Binns]   \n",
       "2                                                                                                                                                       [Noor Almaadeed, Krishna Busawon, Ahmed Bouridane, Richard Binns, Ian Elliot]   \n",
       "3  [Yuankui Leng, Bin Liu, Jun Zhou, Liyang Shao, Jinhui Yuan, Xinyu Fan, Shengpeng Wan, Tao Wu, Juan Liu, Richard Binns, Yong Qing Fu, Wai Pang Ng, Gerald Farrell, Yuliya Semenova, Hengyi Xu, Yonghua Xiong, Xingdao He, Qiang Wu]   \n",
       "4                                                                   [Qian Zhang, Ran Tao, Dongyang Chen, Jin Xie, Hamdi Torun, Linzi E Dodd, Jingting Luo, Chen Fu, Jethro Vernon, Pep Canyelles-Pericas, Richard Binns, Yongqing Fu]   \n",
       "\n",
       "    PaperID  \n",
       "0  33461872  \n",
       "1  24993085  \n",
       "2  84642498  \n",
       "3  60436808  \n",
       "4  95657531  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39f50492-9fc9-4d48-9e6c-8f566ea44da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Academic</th>\n",
       "      <th>Affiliation</th>\n",
       "      <th>Citations</th>\n",
       "      <th>h-index</th>\n",
       "      <th>i10-index</th>\n",
       "      <th>Citations in 1996</th>\n",
       "      <th>Citations in 1997</th>\n",
       "      <th>Citations in 1998</th>\n",
       "      <th>Citations in 1999</th>\n",
       "      <th>Citations in 2000</th>\n",
       "      <th>Citations in 2001</th>\n",
       "      <th>Citations in 2002</th>\n",
       "      <th>Citations in 2003</th>\n",
       "      <th>Citations in 2004</th>\n",
       "      <th>Citations in 2005</th>\n",
       "      <th>Citations in 2006</th>\n",
       "      <th>Citations in 2007</th>\n",
       "      <th>Citations in 2008</th>\n",
       "      <th>Citations in 2009</th>\n",
       "      <th>Citations in 2010</th>\n",
       "      <th>Citations in 2011</th>\n",
       "      <th>Citations in 2012</th>\n",
       "      <th>Citations in 2013</th>\n",
       "      <th>Citations in 2014</th>\n",
       "      <th>Citations in 2015</th>\n",
       "      <th>Citations in 2016</th>\n",
       "      <th>Citations in 2017</th>\n",
       "      <th>Citations in 2018</th>\n",
       "      <th>Citations in 2019</th>\n",
       "      <th>Citations in 2020</th>\n",
       "      <th>Citations in 2021</th>\n",
       "      <th>Citations in 2022</th>\n",
       "      <th>Citations in 2023</th>\n",
       "      <th>AcademicID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Richard Binns</td>\n",
       "      <td>Northumbria University</td>\n",
       "      <td>626</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>64</td>\n",
       "      <td>90</td>\n",
       "      <td>134</td>\n",
       "      <td>147</td>\n",
       "      <td>99</td>\n",
       "      <td>80858125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clare Watt</td>\n",
       "      <td>Northumbria University</td>\n",
       "      <td>2576</td>\n",
       "      <td>31</td>\n",
       "      <td>71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>41</td>\n",
       "      <td>53</td>\n",
       "      <td>62</td>\n",
       "      <td>72</td>\n",
       "      <td>79</td>\n",
       "      <td>85</td>\n",
       "      <td>81</td>\n",
       "      <td>104</td>\n",
       "      <td>112</td>\n",
       "      <td>104</td>\n",
       "      <td>127</td>\n",
       "      <td>169</td>\n",
       "      <td>224</td>\n",
       "      <td>276</td>\n",
       "      <td>343</td>\n",
       "      <td>382</td>\n",
       "      <td>163</td>\n",
       "      <td>71298827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Martin Bees</td>\n",
       "      <td>University of York</td>\n",
       "      <td>1923</td>\n",
       "      <td>26</td>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>46</td>\n",
       "      <td>54</td>\n",
       "      <td>53</td>\n",
       "      <td>42</td>\n",
       "      <td>57</td>\n",
       "      <td>67</td>\n",
       "      <td>59</td>\n",
       "      <td>96</td>\n",
       "      <td>134</td>\n",
       "      <td>94</td>\n",
       "      <td>124</td>\n",
       "      <td>107</td>\n",
       "      <td>125</td>\n",
       "      <td>124</td>\n",
       "      <td>142</td>\n",
       "      <td>149</td>\n",
       "      <td>158</td>\n",
       "      <td>112</td>\n",
       "      <td>25193792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Henning Bostelmann</td>\n",
       "      <td>University of York</td>\n",
       "      <td>351</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>41</td>\n",
       "      <td>55036153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Academic             Affiliation Citations h-index i10-index  \\\n",
       "0       Richard Binns  Northumbria University       626      13        19   \n",
       "0          Clare Watt  Northumbria University      2576      31        71   \n",
       "0         Martin Bees      University of York      1923      26        43   \n",
       "0  Henning Bostelmann      University of York       351      12        13   \n",
       "\n",
       "  Citations in 1996 Citations in 1997 Citations in 1998 Citations in 1999  \\\n",
       "0                 4                 2                 5                 2   \n",
       "0               NaN               NaN               NaN               NaN   \n",
       "0               NaN                 5                 9                13   \n",
       "0               NaN               NaN               NaN               NaN   \n",
       "\n",
       "  Citations in 2000 Citations in 2001 Citations in 2002 Citations in 2003  \\\n",
       "0                 7                 1                 3                 5   \n",
       "0               NaN               NaN               NaN               NaN   \n",
       "0                21                13                27                35   \n",
       "0                 2                 0                 4                 3   \n",
       "\n",
       "  Citations in 2004 Citations in 2005 Citations in 2006 Citations in 2007  \\\n",
       "0               0.0                 1                 0                 1   \n",
       "0               NaN                20                27                41   \n",
       "0                27                24                46                54   \n",
       "0                 4                 1                 7                 6   \n",
       "\n",
       "  Citations in 2008 Citations in 2009 Citations in 2010 Citations in 2011  \\\n",
       "0                 0                 2                 0                 0   \n",
       "0                53                62                72                79   \n",
       "0                53                42                57                67   \n",
       "0                11                16                17                 4   \n",
       "\n",
       "  Citations in 2012 Citations in 2013 Citations in 2014 Citations in 2015  \\\n",
       "0                 1                 3                 1                 4   \n",
       "0                85                81               104               112   \n",
       "0                59                96               134                94   \n",
       "0                13                17                12                24   \n",
       "\n",
       "  Citations in 2016 Citations in 2017 Citations in 2018 Citations in 2019  \\\n",
       "0                 6                14                26                64   \n",
       "0               104               127               169               224   \n",
       "0               124               107               125               124   \n",
       "0                16                12                22                19   \n",
       "\n",
       "  Citations in 2020 Citations in 2021 Citations in 2022 Citations in 2023  \\\n",
       "0                90               134               147                99   \n",
       "0               276               343               382               163   \n",
       "0               142               149               158               112   \n",
       "0                22                36                33                41   \n",
       "\n",
       "   AcademicID  \n",
       "0    80858125  \n",
       "0    71298827  \n",
       "0    25193792  \n",
       "0    55036153  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "academic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e551a14e-7ff9-4506-8eec-dcf8e98b59bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:northumbria-staff-cite]",
   "language": "python",
   "name": "conda-env-northumbria-staff-cite-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
